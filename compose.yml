
services:
  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    restart: unless-stopped
    ports:
      - "0.0.0.0:11434:11434"
    volumes:
      - ./data/llms:/root/.ollama
    runtime: nvidia
    environment:
      NVIDIA_VISIBLE_DEVICES: all

  webui:
    image: ghcr.io/open-webui/open-webui:latest
    container_name: open-webui
    restart: unless-stopped
    ports:
      - "0.0.0.0:80:8080"   # Open WebUI
    environment:
      - OLLAMA_API_BASE_URL=http://ollama:11434
    depends_on:
      - ollama
    volumes:
      - open-webui:/app/backend/data

  chroma:
    image: chromadb/chroma:latest
    container_name: chroma
    ports:
      - "8000:8000"
    volumes:
      # Maps the named Docker volume to the container's data path
      - ./data/chroma:/chroma/chroma 
    environment:
      # Enable persistence
      - IS_PERSISTENT=TRUE
      - PERSIST_DIRECTORY=/chroma/chroma
      # Development-only: allow full data reset
      - ALLOW_RESET=TRUE 
      # Optional: Disable anonymous telemetry
      - ANONYMIZED_TELEMETRY=FALSE

  rag-api:
    build:
      context: raggy
    container_name: rag-api
    # Expose the API to the host for testing and connecting Open WebUI
    ports:
      - "8001:8001" # Maps container port 8000 to host port 8001
    environment:
      # Inject the service names/ports for Python to find them
      - OLLAMA_HOST=http://ollama:11434
      - CHROMA_HOST=http://chroma:8000
      - LLM_MODEL=mistral
      - EMBEDDING_MODEL=nomic-embed-text
    depends_on:
      - ollama
      - chroma
    restart: unless-stopped
volumes:
  ollama_data:
  open-webui: